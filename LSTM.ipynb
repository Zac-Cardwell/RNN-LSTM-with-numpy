{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d504118e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample from the generated dataset:\n",
      "['a', 'a', 'b', 'b', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_dataset(num_sequences=2**8):\n",
    "    \n",
    "    '''\n",
    "    generates a number os sequences to be used as the data set\n",
    "    \n",
    "    Args:\n",
    "    `num_sequences`: the number of sequences to be generated.\n",
    "    \n",
    "    Returns list of sequences \n",
    "    '''\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_sequences): \n",
    "        num_tokens = np.random.randint(1, 12)\n",
    "        sample = ['a'] * num_tokens + ['b'] * num_tokens + ['EOS']\n",
    "        samples.append(sample)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "sequences = create_dataset()\n",
    "\n",
    "print('A sample from the generated dataset:')\n",
    "print(sequences[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9281c7",
   "metadata": {},
   "source": [
    "for this project, one-hot encoding will be used to represent the vocabulary for training. For datasets with larger vocabulary however, one-hot encoding often becomes inefficient because of the size of each sparse vector. To overcome this challenge it is common practice to truncate the vocabulary to contain the most used words and represent the rest with a special symbol, UNK , to define unknown/unimportant words.\n",
    "\n",
    "The create a one-hot encoding for the dataset, we need to assign each word in the vocabulary an index. this can be done by creating two dictionaries: One that gives the index for a given word, and one for the reverse. If a given word is not co ntained in the known vocabulary, it will be given the value of UNK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e77d3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 sentences and 4 unique tokens in dataset (including UNK).\n",
      "\n",
      "The index of 'b' is 1\n",
      "The word corresponding to index 1 is 'b'\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def sequences_to_dicts(sequences):\n",
    "    \n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    # Flatten the dataset\n",
    "    all_words = flatten(sequences)\n",
    "    # Count number of word occurences\n",
    "    word_count = defaultdict(int)\n",
    "    for word in flatten(sequences):\n",
    "        word_count[word] += 1\n",
    "\n",
    "    # Sort by frequency\n",
    "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\n",
    "    # Create a list of all unique words\n",
    "    unique_words = [item[0] for item in word_count]\n",
    "    # Add UNK token to list of words\n",
    "    unique_words.append('UNK')\n",
    "    # Count number of sequences and number of unique words\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
    "    # Create dictionaries to go from word to index and back\n",
    "    # If a word is not in the vocabulary, it will be assigned to token 'UNK'\n",
    "    word_to_idx = defaultdict(lambda: vocab_size-1)\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\n",
    "    # Fill dictionaries\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "\n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
    "\n",
    "\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\n",
    "\n",
    "print(f'{num_sequences} sentences and {len(word_to_idx)} unique tokens in dataset (including UNK).\\n')\n",
    "print('The index of \\'b\\' is', word_to_idx['b'])\n",
    "print(f'The word corresponding to index 1 is \\'{idx_to_word[1]}\\'')\n",
    "\n",
    "assert idx_to_word[word_to_idx['b']] == 'b', \\\n",
    "    'Consistency error: something went wrong in the conversion.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3054df35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ab86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    # Define partition sizes\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "\n",
    "    # Split sequences into partitions\n",
    "    sequences_train = sequences[:num_train]\n",
    "    sequences_val = sequences[num_train:num_train+num_val]\n",
    "    sequences_test = sequences[-num_test:]\n",
    "\n",
    "    def get_inputs_targets_from_sequences(sequences):\n",
    "        # Define empty lists\n",
    "        inputs, targets = [], []\n",
    "        \n",
    "        # Append inputs and targets s.t. both lists contain L-1 words of a sentence of length L\n",
    "        # but targets are shifted right by one so that we can predict the next word\n",
    "        for sequence in sequences:\n",
    "            inputs.append(sequence[:-1])\n",
    "            targets.append(sequence[1:])\n",
    "            \n",
    "        return inputs, targets\n",
    "\n",
    "    # Get inputs and targets for each partition\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
    "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
    "\n",
    "    # Create datasets\n",
    "    training_set = dataset_class(inputs_train, targets_train)\n",
    "    validation_set = dataset_class(inputs_val, targets_val)\n",
    "    test_set = dataset_class(inputs_test, targets_test)\n",
    "\n",
    "    return training_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0630dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 samples in the training set.\n",
      "25 samples in the validation set.\n",
      "25 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "training_set, validation_set, test_set = create_datasets(sequences, Dataset)\n",
    "\n",
    "print(f'{len(training_set)} samples in the training set.')\n",
    "print(f'{len(validation_set)} samples in the validation set.')\n",
    "print(f'{len(test_set)} samples in the test set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab764f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    # Set the appropriate element to one\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sequence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    # Encode each word in the sentence\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4774e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoding of 'a' has shape (4,).\n",
      "one-hot encoding of 'a b' has shape (2, 4, 1).\n"
     ]
    }
   ],
   "source": [
    "test_word = one_hot_encode(word_to_idx['a'], vocab_size)\n",
    "print(f'one-hot encoding of \\'a\\' has shape {test_word.shape}.')\n",
    "\n",
    "test_sentence = one_hot_encode_sequence(['a', 'b'], vocab_size)\n",
    "print(f'one-hot encoding of \\'a b\\' has shape {test_sentence.shape}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fae539df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_f: (50, 54)\n",
      "W_i: (50, 54)\n",
      "W_g: (50, 54)\n",
      "W_o: (50, 54)\n",
      "W_v: (4, 50)\n",
      "b_i: (50, 1)\n",
      "b_g: (50, 1)\n",
      "b_o: (50, 1)\n",
      "b_v: (50, 1)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 50 # Number of dimensions in the hidden state\n",
    "vocab_size  = len(word_to_idx) # Size of the vocabulary used\n",
    "z_size = hidden_size + vocab_size # Size of concatenated hidden + input vector\n",
    "\n",
    "def init_orthogonal(param):\n",
    "    # Initializes weight parameters orthogonally\n",
    "    if param.ndim < 2:\n",
    "        raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "    \n",
    "    rows, cols = param.shape\n",
    "    new_param = np.random.randn(rows, cols)\n",
    "    if rows < cols:\n",
    "        new_param = new_param.T    \n",
    "    \n",
    "    # Compute QR factorization\n",
    "    q, r = np.linalg.qr(new_param)\n",
    "    \n",
    "    # Make Q uniform \n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        q = q.T\n",
    "    \n",
    "    new_param = q\n",
    "    return new_param\n",
    "\n",
    "\n",
    "def init_lstm(hidden_size, vocab_size, z_size):\n",
    "    \"\"\"\n",
    "    Initializes LSTM network.\n",
    "    \n",
    "    Args:\n",
    "     `hidden_size`: the dimensions of the hidden state\n",
    "     `vocab_size`: the dimensions of vocabulary\n",
    "     `z_size`: the dimensions of the concatenated input \n",
    "    \"\"\"\n",
    "    # Weight matrix (forget gate)\n",
    "    W_f = np.zeros((hidden_size, z_size))\n",
    "    \n",
    "    # Bias for forget gate\n",
    "    b_f = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Weight matrix (input gate)\n",
    "    W_i = np.zeros((hidden_size, z_size))\n",
    "    \n",
    "    # Bias for input gate\n",
    "    b_i = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Weight matrix (candidate)\n",
    "    W_g = np.zeros((hidden_size, z_size))\n",
    "    \n",
    "    # Bias for candidate\n",
    "    b_g = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Weight matrix of the output gate\n",
    "    W_o = np.zeros((hidden_size, z_size))\n",
    "    \n",
    "    # Bias for output gate\n",
    "    b_o = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Weight matrix relating the hidden-state to the output\n",
    "    W_v = np.zeros((vocab_size, hidden_size))\n",
    "    \n",
    "    # Bias for logits\n",
    "    b_v = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    # Initialize weights according to https://arxiv.org/abs/1312.6120\n",
    "    W_f = init_orthogonal(W_f)\n",
    "    W_i = init_orthogonal(W_i)\n",
    "    W_g = init_orthogonal(W_g)\n",
    "    W_o = init_orthogonal(W_o)\n",
    "    W_v = init_orthogonal(W_v)\n",
    "\n",
    "    return W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v\n",
    "\n",
    "\n",
    "params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)\n",
    "print('W_f:', params[0].shape)\n",
    "print('W_i:', params[1].shape)\n",
    "print('W_g:', params[2].shape)\n",
    "print('W_o:', params[3].shape)\n",
    "print('W_v:', params[4].shape)\n",
    "print('b_i:', params[5].shape)\n",
    "print('b_g:', params[6].shape)\n",
    "print('b_o:', params[7].shape)\n",
    "print('b_v:', params[8].shape)\n",
    "\n",
    "for param in params:\n",
    "    assert param.ndim == 2, \\\n",
    "        'all parameters should be 2-dimensional '\\\n",
    "        '(hint: a dimension can simply have size 1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f26f7440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise sigmoid activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = 1 / (1 + np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return f * (1 - f)\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8276947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12 \n",
    "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return 1-f**2\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7986282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the softmax for an array x.\n",
    "    \n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        pass\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c817bcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'EOS', 'EOS', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n"
     ]
    }
   ],
   "source": [
    "def forward(inputs, h_prev, C_prev, p):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    h_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    C_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    p -- python list containing:\n",
    "                        W_f -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_f -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        W_i -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_i -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        W_g -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_g --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        W_o -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_o --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        W_v -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_v, n_a)\n",
    "                        b_v -- Bias relating the hidden-state to the output, numpy array of shape (n_v, 1)\n",
    "    Returns:\n",
    "    z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s -- lists of size m containing the computations in each forward pass\n",
    "    outputs -- prediction at timestep \"t\", numpy array of shape (n_v, m)\n",
    "    \"\"\"\n",
    "    assert h_prev.shape == (hidden_size, 1)\n",
    "    assert C_prev.shape == (hidden_size, 1)\n",
    "\n",
    "    # unpacking parameters\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "    \n",
    "    # Save a list of computations for each of the components in the LSTM\n",
    "    x_s, z_s, f_s, i_s,  = [], [] ,[], []\n",
    "    g_s, C_s, o_s, h_s = [], [] ,[], []\n",
    "    v_s, output_s =  [], [] \n",
    "    \n",
    "    # Append the initial cell and hidden state to their respective lists\n",
    "    h_s.append(h_prev)\n",
    "    C_s.append(C_prev)\n",
    "    \n",
    "    for x in inputs:\n",
    "        \n",
    "        # Concatenate input and hidden state\n",
    "        z = np.row_stack((h_prev, x))\n",
    "        z_s.append(z)\n",
    "        \n",
    "        # Calculate forget gate\n",
    "        # YOUR CODE HERE!\n",
    "        f = sigmoid(np.dot(W_f, z)) + b_f\n",
    "        f_s.append(f)\n",
    "        \n",
    "        # Calculate input gate\n",
    "        i = sigmoid(np.dot(W_i, z)) + b_i\n",
    "        i_s.append(i)\n",
    "        \n",
    "        # Calculate candidate\n",
    "        g = tanh(np.dot(W_g, z) + b_g)\n",
    "        g_s.append(g)\n",
    "        \n",
    "        # Calculate memory state\n",
    "        C_prev = f * C_prev + i * g\n",
    "        C_s.append(C_prev)\n",
    "        \n",
    "        # Calculate output gate\n",
    "        o =  sigmoid(np.dot(W_o, z)) + b_o\n",
    "        o_s.append(o)\n",
    "        \n",
    "        # Calculate hidden state\n",
    "        h_prev = o * tanh(C_prev)\n",
    "        h_s.append(h_prev)\n",
    "\n",
    "        # Calculate logits\n",
    "        v = np.dot(W_v, h_prev) + b_v\n",
    "        v_s.append(v)\n",
    "        \n",
    "        # Calculate softmax\n",
    "        output = softmax(v)\n",
    "        output_s.append(output)\n",
    "\n",
    "    return z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s\n",
    "\n",
    "\n",
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "h = np.zeros((hidden_size, 1))\n",
    "c = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "\n",
    "output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]\n",
    "print('Input sentence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d44823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get a loss of:\n",
      "4.7019778386592606\n"
     ]
    }
   ],
   "source": [
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    \"\"\"\n",
    "    Clips gradients to have a maximum norm of `max_norm`.\n",
    "    This is to prevent the exploding gradients problem.\n",
    "    \"\"\" \n",
    "    # Set the maximum of the norm to be of type float\n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Calculate clipping coeficient\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def backward(z, f, i, g, C, o, h, v, outputs, targets, p = params):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z -- your concatenated input data  as a list of size m.\n",
    "    f -- your forget gate computations as a list of size m.\n",
    "    i -- your input gate computations as a list of size m.\n",
    "    g -- your candidate computations as a list of size m.\n",
    "    C -- your Cell states as a list of size m+1.\n",
    "    o -- your output gate computations as a list of size m.\n",
    "    h -- your Hidden state computations as a list of size m+1.\n",
    "    v -- your logit computations as a list of size m.\n",
    "    outputs -- your outputs as a list of size m.\n",
    "    targets -- your targets as a list of size m.\n",
    "    p -- python list containing:\n",
    "                        W_f -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_f -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        W_i -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_i -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        W_g -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_g --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        W_o -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_o --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        W_v -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_v, n_a)\n",
    "                        b_v -- Bias relating the hidden-state to the output, numpy array of shape (n_v, 1)\n",
    "    Returns:\n",
    "    loss -- crossentropy loss for all elements in output\n",
    "    grads -- lists of gradients of every element in p\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack parameters\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "\n",
    "    # Initialize gradients as zero\n",
    "    W_f_d = np.zeros_like(W_f)\n",
    "    b_f_d = np.zeros_like(b_f)\n",
    "\n",
    "    W_i_d = np.zeros_like(W_i)\n",
    "    b_i_d = np.zeros_like(b_i)\n",
    "\n",
    "    W_g_d = np.zeros_like(W_g)\n",
    "    b_g_d = np.zeros_like(b_g)\n",
    "\n",
    "    W_o_d = np.zeros_like(W_o)\n",
    "    b_o_d = np.zeros_like(b_o)\n",
    "\n",
    "    W_v_d = np.zeros_like(W_v)\n",
    "    b_v_d = np.zeros_like(b_v)\n",
    "    \n",
    "    # Set the next cell and hidden state equal to zero\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    dC_next = np.zeros_like(C[0])\n",
    "        \n",
    "    # Track loss\n",
    "    loss = 0\n",
    "    \n",
    "    for t in reversed(range(len(outputs))):\n",
    "        \n",
    "        # Compute the cross entropy\n",
    "        loss += -np.mean(np.log(outputs[t]) * targets[t])\n",
    "        # Get the previous hidden cell state\n",
    "        C_prev= C[t-1]\n",
    "        \n",
    "        # Compute the derivative of the relation of the hidden-state to the output gate\n",
    "        dv = np.copy(outputs[t])\n",
    "        dv[np.argmax(targets[t])] -= 1\n",
    "\n",
    "        # Update the gradient of the relation of the hidden-state to the output gate\n",
    "        W_v_d += np.dot(dv, h[t].T)\n",
    "        b_v_d += dv\n",
    "\n",
    "        # Compute the derivative of the hidden state and output gate\n",
    "        dh = np.dot(W_v.T, dv)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(C[t])\n",
    "        do = sigmoid(o[t], derivative=True)*do\n",
    "        \n",
    "        # Update the gradients with respect to the output gate\n",
    "        W_o_d += np.dot(do, z[t].T)\n",
    "        b_o_d += do\n",
    "\n",
    "        # Compute the derivative of the cell state and candidate g\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * o[t] * tanh(tanh(C[t]), derivative=True)\n",
    "        dg = dC * i[t]\n",
    "        dg = tanh(g[t], derivative=True) * dg\n",
    "        \n",
    "        # Update the gradients with respect to the candidate\n",
    "        W_g_d += np.dot(dg, z[t].T)\n",
    "        b_g_d += dg\n",
    "\n",
    "        # Compute the derivative of the input gate and update its gradients\n",
    "        di = dC * g[t]\n",
    "        di = sigmoid(i[t], True) * di\n",
    "        W_i_d += np.dot(di, z[t].T)\n",
    "        b_i_d += di\n",
    "\n",
    "        # Compute the derivative of the forget gate and update its gradients\n",
    "        df = dC * C_prev\n",
    "        df = sigmoid(f[t]) * df\n",
    "        W_f_d += np.dot(df, z[t].T)\n",
    "        b_f_d += df\n",
    "\n",
    "        # Compute the derivative of the input and update the gradients of the previous hidden and cell state\n",
    "        dz = (np.dot(W_f.T, df)\n",
    "             + np.dot(W_i.T, di)\n",
    "             + np.dot(W_g.T, dg)\n",
    "             + np.dot(W_o.T, do))\n",
    "        dh_prev = dz[:hidden_size, :]\n",
    "        dC_prev = f[t] * dC\n",
    "        \n",
    "    grads= W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d\n",
    "    \n",
    "    # Clip gradients\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "# Perform a backward pass\n",
    "loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "\n",
    "print('We get a loss of:')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7e14c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, lr=1e-3):\n",
    "    # Take a step\n",
    "    for param, grad in zip(params, grads):\n",
    "        param -= lr * grad \n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be8c4a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 4.156733540367533, validation loss: 4.08704647829934\n",
      "Epoch 50, training loss: 2.3383270598421393, validation loss: 2.308388965842491\n",
      "Epoch 100, training loss: 1.6697724251345192, validation loss: 1.6585636814266909\n",
      "Epoch 150, training loss: 1.4120583028083307, validation loss: 1.4008590615585084\n",
      "Epoch 200, training loss: 1.3140430586356788, validation loss: 1.3002962801669784\n",
      "Epoch 250, training loss: 1.273157986351839, validation loss: 1.253658767160099\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTcElEQVR4nO3dd3wUdf7H8dek9xBqggkQjiI1ICBEikiHk4MTFRERRFGUoqfeIVbUn2IDEQscNlQUUAKIRxEQQpEiICUiIEowCImhJiSEhCTz+2PMQighgSSz2byfj8c89rszs7ufndszb2a+8/0apmmaiIiIiLgIN7sLEBERESlOCjciIiLiUhRuRERExKUo3IiIiIhLUbgRERERl6JwIyIiIi5F4UZERERciofdBZS23NxcDh06RGBgIIZh2F2OiIiIFIJpmpw8eZLq1avj5lbwuZlyF24OHTpERESE3WWIiIjIFThw4ADh4eEF7lPuwk1gYCBgHZygoCCbqxEREZHCSE1NJSIiwvF3vCDlLtzkXYoKCgpSuBERESljCtOlRB2KRURExKUo3IiIiIhLUbgRERERl1Lu+tyIiEjxys3NJSsry+4yxAV4eXld9jbvwlC4ERGRK5aVlUV8fDy5ubl2lyIuwM3NjcjISLy8vK7qfRRuRETkipimSWJiIu7u7kRERBTLv7il/MobZDcxMZEaNWpc1UC7CjciInJFsrOzOXXqFNWrV8fPz8/ucsQFVKlShUOHDpGdnY2np+cVv49itoiIXJGcnByAq76EIJIn77eU99u6Ugo3IiJyVTRPnxSX4votKdyIiIiIS1G4EREREZeicCMiInKVOnbsyCOPPFLo/ffv349hGGzbtq3EagKIjY3FMAxOnDhRop/jbHS3VHFKSYH9+yEqyu5KRETkIi7Xp2Pw4MFMnz69yO87d+7cIt3dExERQWJiIpUrVy7yZ8nlKdwUl7g4aNoUKlaEI0dAHexERJxOYmKioz179myeffZZ9uzZ41jn6+ubb/8zZ84UKrRUrFixSHW4u7sTGhpapNdI4emyVHGpVw88PODYMfjjD7urEREpfaYJ6en2LKZZqBJDQ0MdS3BwMIZhOJ6fPn2aChUq8OWXX9KxY0d8fHyYMWMGR48eZcCAAYSHh+Pn50eTJk2YOXNmvvc9/7JUrVq1ePnllxk6dCiBgYHUqFGDadOmObaff1kq7/LRd999R8uWLfHz8+OGG27IF7wA/u///o+qVasSGBjIfffdxxNPPEGzZs2K9D9TTEwMjRo1wtvbm1q1ajFhwoR829977z3q1q2Lj48P1apV49Zbb3VsmzNnDk2aNMHX15dKlSrRpUsX0tPTi/T5pUHhprh4e0ODBla7hK+hiog4pVOnICDAnuXUqWL7GmPGjGH06NHs2rWL7t27c/r0aVq0aMH//vc/fvrpJ+6//34GDRrExo0bC3yfCRMm0LJlS7Zu3cpDDz3Egw8+yO7duwt8zVNPPcWECRPYvHkzHh4eDB061LHt888/56WXXuLVV19ly5Yt1KhRgylTphTpu23ZsoXbb7+dO+64g7i4OMaNG8czzzzjuBS3efNmRo8ezQsvvMCePXtYsmQJHTp0AKyzXgMGDGDo0KHs2rWL2NhYbrnlFsxCBstSZZYzKSkpJmCmpKQU/5sPGmSaYJovvFD87y0i4mQyMjLMn3/+2czIyLBWpKVZ/w20Y0lLK3L9H3/8sRkcHOx4Hh8fbwLmpEmTLvvaXr16mY899pjj+Y033mg+/PDDjuc1a9Y077rrLsfz3Nxcs2rVquaUKVPyfdbWrVtN0zTNlStXmoC5fPlyx2sWLlxoAo7j27p1a3PEiBH56mjbtq0ZFRV1yTrz3vf48eOmaZrmnXfeaXbt2jXfPv/+97/Nhg0bmqZpmjExMWZQUJCZmpp6wXtt2bLFBMz9+/df8vOu1gW/qXMU5e+3ztwUp7xTgzpzIyLlkZ8fpKXZsxTj9A8tW7bM9zwnJ4eXXnqJpk2bUqlSJQICAli6dCkJCQkFvk/Tpk0d7bzLX8nJyYV+TVhYGIDjNXv27OH666/Pt//5zy9n165dtG3bNt+6tm3bsnfvXnJycujatSs1a9akdu3aDBo0iM8//5xTf50Vi4qKonPnzjRp0oTbbruN999/n+PHjxfp80uLwk1xygs327fbWoaIiC0MA/z97VmK8SYOf3//fM8nTJjAm2++yX/+8x9WrFjBtm3b6N69O1lZWQW+z/kdkQ3DuOzs6ee+Ju/OrnNfc/7dXmYRLwmZplngewQGBvLjjz8yc+ZMwsLCePbZZ4mKiuLEiRO4u7uzbNkyFi9eTMOGDXn77bepX78+8fHxRaqhNCjcFKe8W8B/+w1SU+2tRUREisWaNWvo06cPd911F1FRUdSuXZu9e/eWeh3169fnhx9+yLdu8+bNRXqPhg0bsnbt2nzr1q1bR7169XB3dwfAw8ODLl268Nprr7Fjxw7279/PihUrACtctW3blueff56tW7fi5eXFvHnzruJblQzdCl6MjpiV+L1qT1okL4YdO6BdO7tLEhGRq1SnTh1iYmJYt24dISEhTJw4kaSkJBrk3URSSkaNGsWwYcNo2bIlN9xwA7Nnz2bHjh3Url270O/x2GOP0apVK1588UX69+/P+vXreeedd3jvvfcA+N///se+ffvo0KEDISEhLFq0iNzcXOrXr8/GjRv57rvv6NatG1WrVmXjxo0cPny41I9DYSjcFJMdO6wTNyGeszlKEMa2bQo3IiIu4JlnniE+Pp7u3bvj5+fH/fffT9++fUlJSSnVOgYOHMi+fft4/PHHOX36NLfffjtDhgy54GxOQa677jq+/PJLnn32WV588UXCwsJ44YUXGDJkCAAVKlRg7ty5jBs3jtOnT1O3bl1mzpxJo0aN2LVrF6tXr2bSpEmkpqZSs2ZNJkyYQM+ePUvoG185wyzqBbsyLjU1leDgYFJSUggKCiq2983Ksu5GPHMG9lOTmvd1g/ffL7b3FxFxNqdPnyY+Pp7IyEh8fHzsLqdc6tq1K6GhoXz22Wd2l1IsCvpNFeXvt87cFBMvL2jY0OpLvI1m1NQdUyIiUoxOnTrF1KlT6d69O+7u7sycOZPly5ezbNkyu0tzOupQXIzybpbaSnNrOobsbFvrERER12EYBosWLaJ9+/a0aNGCb775hpiYGLp06WJ3aU5HZ26KUfPm8MknsM29JWRmwp490KiR3WWJiIgL8PX1Zfny5XaXUSbozE0xcpy5cf9rACiNdyMiIlLqFG6KUV64ScgK5RghGqlYRETEBk4TbsaPH49hGPlmVb2YVatW0aJFC3x8fKhduzZTp04tnQILITgYIiOt9jaaKdyIiIjYwCnCzaZNm5g2bVq+OTUuJj4+nl69etG+fXu2bt3Kk08+yejRo4mJiSmlSi+veXPr0RFuyted9iIiIrazPdykpaUxcOBA3n//fUJCQgrcd+rUqdSoUYNJkybRoEED7rvvPoYOHcobb7xRStVenqPfjXEdHD4MBw/aWo+IiEh5Y3u4GTFiBH//+98LdSvb+vXr6datW7513bt3Z/PmzZw5c+air8nMzCQ1NTXfUpIcZ268WluNLVtK9PNERKT0dezYMV83ilq1ajFp0qQCX2MYBvPnz7/qzy6u9ynIuHHjaJb3r/UyyNZwM2vWLH788UfGjx9fqP2TkpKoVq1avnXVqlUjOzubI0eOXPQ148ePJzg42LFERERcdd0Fyfst7MqqTQY+CjciIk6kd+/el/zH9Pr16zEMgx9//LHI77tp0ybuv//+qy0vn0sFjMTERKec8sCZ2BZuDhw4wMMPP8yMGTOKNGz3paZqP399nrFjx5KSkuJYDhw4cOVFF8I110DlypBjurOTRgo3IiJO5N5772XFihX8/vvvF2z76KOPaNasGdddd12R37dKlSr4+fkVR4mXFRoaire3d6l8VlllW7jZsmULycnJtGjRAg8PDzw8PFi1ahWTJ0/Gw8ODnJycC14TGhpKUlJSvnXJycl4eHhQqVKli36Ot7c3QUFB+ZaSZBjnjVS8ZYs6FYuIOImbb76ZqlWrMn369HzrT506xezZs7n33ns5evQoAwYMIDw8HD8/P5o0acLMmTMLfN/zL0vt3buXDh064OPjQ8OGDS86RcKYMWOoV68efn5+1K5dm2eeecbRxWL69Ok8//zzbN++HcMwMAzDUfP5l6Xi4uLo1KkTvr6+VKpUifvvv5+0tDTH9iFDhtC3b1/eeOMNwsLCqFSpEiNGjLhkd46Lyc3N5YUXXiA8PBxvb2+aNWvGkiVLHNuzsrIYOXIkYWFh+Pj4UKtWrXxXZcaNG0eNGjXw9vamevXqjB49utCffSVsG6G4c+fOxMXF5Vt3zz33cO211zJmzBjc3d0veE10dDTffPNNvnVLly6lZcuWeHp6lmi9RdGsGSxfDttoDn9+AIcOWad0RERcmGnCqVP2fLafn/WPy8vx8PDg7rvvZvr06Tz77LOOs/5fffUVWVlZDBw4kFOnTtGiRQvGjBlDUFAQCxcuZNCgQdSuXZvWrVtf9jNyc3O55ZZbqFy5Mhs2bCA1NfWiw5wEBgYyffp0qlevTlxcHMOGDSMwMJD//Oc/9O/fn59++oklS5Y4RiUODg6+4D1OnTpFjx49aNOmDZs2bSI5OZn77ruPkSNH5gtwK1euJCwsjJUrV/Lrr7/Sv39/mjVrxrBhwy5/0IC33nqLCRMm8N///pfmzZvz0Ucf8Y9//IOdO3dSt25dJk+ezIIFC/jyyy+pUaMGBw4ccFwpmTNnDm+++SazZs2iUaNGJCUlsb2kB7k1nciNN95oPvzww47nTzzxhDlo0CDH83379pl+fn7mv/71L/Pnn382P/zwQ9PT09OcM2dOoT8jJSXFBMyUlJTiLD2fzz83TTDNaN+tVuPrr0vss0RE7JKRkWH+/PPPZkZGhmmappmWZv0nz44lLa3wde/atcsEzBUrVjjWdejQwRwwYMAlX9OrVy/zscceczw//+9VzZo1zTfffNM0TdP89ttvTXd3d/PAgQOO7YsXLzYBc968eZf8jNdee81s0aKF4/lzzz1nRkVFXbDfue8zbdo0MyQkxEw75wAsXLjQdHNzM5OSkkzTNM3BgwebNWvWNLOzsx373HbbbWb//v0vWcv5n129enXzpZdeyrdPq1atzIceesg0TdMcNWqU2alTJzM3N/eC95owYYJZr149Mysr65Kfl+f839S5ivL32/a7pQqSmJhIQkKC43lkZCSLFi0iNjaWZs2a8eKLLzJ58mT69etnY5UXyrsstf1MA3JwU78bEREncu2113LDDTfw0UcfAfDbb7+xZs0ahg4dCkBOTg4vvfQSTZs2pVKlSgQEBLB06dJ8f48KsmvXLmrUqEF4eLhjXXR09AX7zZkzh3bt2hEaGkpAQADPPPNMoT/j3M+KiorC39/fsa5t27bk5uayZ88ex7pGjRrluyISFhZGcnJyoT4jNTWVQ4cO0bZt23zr27Zty65duwDr0te2bduoX78+o0ePZunSpY79brvtNjIyMqhduzbDhg1j3rx5ZJfwxNJONXFmbGxsvufnXxMFuPHGG6+oJ3tpql/fOkV66pQ3v1CPBgo3IlIO+PnBOV09Sv2zi+Lee+9l5MiRvPvuu3z88cfUrFmTzp07AzBhwgTefPNNJk2aRJMmTfD39+eRRx4hKyurUO9tXqSf5fk3vWzYsIE77riD559/nu7duxMcHMysWbOYMGFCkb6HaZqXvKHm3PXnd90wDIPc3NwifdbFbujJW3fdddcRHx/P4sWLWb58ObfffjtdunRhzpw5REREsGfPHpYtW8by5ct56KGHeP3111m1alWJdSlx6jM3ZZW7+9nxbjbTUmduRKRcMAzw97dnKUx/m3PdfvvtuLu788UXX/DJJ59wzz33OP5Qr1mzhj59+nDXXXcRFRVF7dq12bt3b6Hfu2HDhiQkJHDo0CHHuvXr1+fb5/vvv6dmzZo89dRTtGzZkrp1615wB5eXl9dFb645/7O2bdtGenp6vvd2c3OjXr16ha65IEFBQVSvXp21a9fmW79u3ToaNGiQb7/+/fvz/vvvM3v2bGJiYjh27BhgzWj+j3/8g8mTJxMbG8v69esv6HdbnBRuSkjLvyYG30wrSEqyOhWLiIhTCAgIoH///jz55JMcOnSIIUOGOLbVqVOHZcuWsW7dOnbt2sUDDzxwwZ26BenSpQv169fn7rvvZvv27axZs4annnoq3z516tQhISGBWbNm8dtvvzF58mTmzZuXb59atWoRHx/Ptm3bOHLkCJmZmRd81sCBA/Hx8WHw4MH89NNPrFy5klGjRjFo0KALxoW7Gv/+97959dVXmT17Nnv27OGJJ55g27ZtPPzwwwCODsO7d+/ml19+4auvviI0NJQKFSowffp0PvzwQ3766Sf27dvHZ599hq+vLzVr1iy2+s6ncFNCHOHGt73V0NkbERGncu+993L8+HG6dOlCjRo1HOufeeYZrrvuOrp3707Hjh0JDQ2lb9++hX5fNzc35s2bR2ZmJtdffz333XcfL730Ur59+vTpw7/+9S9GjhxJs2bNWLduHc8880y+ffr160ePHj246aabqFKlykVvR/fz8+Pbb7/l2LFjtGrViltvvZXOnTvzzjvvFO1gXMbo0aN57LHHeOyxx2jSpAlLlixhwYIF1K1bF7DC4quvvkrLli1p1aoV+/fvZ9GiRbi5uVGhQgXef/992rZtS9OmTfnuu+/45ptvLjmES3EwzItdHHRhqampBAcHk5KSUqJj3uzeDQ0agJ9HJinZ/ng89zSMG1dinyciUtpOnz5NfHw8kZGRRRqMVeRSCvpNFeXvt87clJB69SAgAE5le7Oba3XmRkREpJQo3JQQNzfIG8FbnYpFRERKj8JNCcrXqTgxEQ4etLcgERGRckDhpgTlhZstfu2sxg8/2FeMiIhIOaFwU4Lyws22zAacwQM2brS3IBGRElDO7kuRElRcvyWFmxL0t79BUBCczvHiZxrqzI2IuJS84fwLO3KvyOXk/ZYuNnl2UTjV9Auuxs0NWrSAlSutTsVRm76EnBxrCGMRkTLOw8MDPz8/Dh8+jKenJ25u+veyXLnc3FwOHz6Mn58fHh5XF08UbkpYy5ZWuNni0YZ70z6CXbugcWO7yxIRuWqGYRAWFkZ8fPwFUweIXAk3Nzdq1KhxyfmyCkvhpoSdHam4HZzEujSlcCMiLsLLy4u6devq0pQUCy8vr2I5A6hwU8Lyws32U3XJwhOvjRth6FB7ixIRKUZubm4aoVicii6QlrDISKhUCbJyPNhOlO6YEhERKWEKNyXMMKB1a6u9gTbw009wztT0IiIiUrwUbkpBXrjZ6NvRulvqxx9trUdERMSVKdyUgjZtrMcNbjdYDV2aEhERKTEKN6Xg+uutx9/SwzhCJYUbERGREqRwUwoqVID69a32RlprpGIREZESpHBTSvIuTW2kDSQkwKFD9hYkIiLiohRuSomjU3FAZ6uxbp19xYiIiLgwhZtS4jhzk9WMXAz4/nt7CxIREXFRCjelpEkT8PWFlCw/fqGeztyIiIiUEIWbUuLhcXYqhg20sca6OXXK3qJERERckMJNKXL0u/G7CbKzYfNmewsSERFxQQo3pcgxmJ9XB6uhfjciIiLFTuGmFOWduYlLrUk6fgo3IiIiJUDhphSFh0NEBOTkulmD+a1fD7m5dpclIiLiUhRuSlm7dtbj9x4d4dgx2LPH1npERERcjcJNKWvb1npcG9jDauiWcBERkWKlcFPK8s7crE9vSg5u6ncjIiJSzBRuSlnjxhAUBCezfIijicKNiIhIMVO4KWXu7hAdbbXX0g5++QWSk+0tSkRExIUo3Nggr9/N98G9rMaaNfYVIyIi4mIUbmyQ1+9mbc5fp3BWrbKvGBERERejcGOD66+3Lk/9kRZCAhEKNyIiIsVI4cYG/v5w3XVWey3tIC7OGvNGRERErprCjU0c/W4q3AymqX43IiIixUThxiaOfjduf02iqUtTIiIixULhxiZ5Z27ijl/DCYIVbkRERIqJwo1NQkOhbl0wTYM1tIdt2yAlxe6yREREyjyFGxt17Gg9xgb3tWYHX7vWznJERERcgsKNjfLCzSqPzn81dGlKRETkainc2OjGG63HrcdqqN+NiIhIMVG4sdE111j9bnJNN2u8my1b4ORJu8sSEREp02wNN1OmTKFp06YEBQURFBREdHQ0ixcvvuT+sbGxGIZxwbJ79+5SrLp4OfrdBPWBnBxYvdrWekRERMo6W8NNeHg4r7zyCps3b2bz5s106tSJPn36sHPnzgJft2fPHhITEx1L3bp1S6ni4ucIN15drcZ339lWi4iIiCvwsPPDe/fune/5Sy+9xJQpU9iwYQONGjW65OuqVq1KhQoVSri60uHod3PU6ndTYflyewsSEREp45ymz01OTg6zZs0iPT2d6OjoAvdt3rw5YWFhdO7cmZUrVxa4b2ZmJqmpqfkWZ3LNNVCnzjn9buLi4M8/7S5LRESkzLI93MTFxREQEIC3tzfDhw9n3rx5NGzY8KL7hoWFMW3aNGJiYpg7dy7169enc+fOrC6gn8r48eMJDg52LBERESX1Va6Y49JUldutxooVttUiIiJS1hmmaZp2FpCVlUVCQgInTpwgJiaGDz74gFWrVl0y4Jyvd+/eGIbBggULLro9MzOTzMxMx/PU1FQiIiJISUkhKCioWL7D1fr8c7jrLmhR7QCb/6wBQ4fChx/aXZaIiIjTSE1NJTg4uFB/v20/c+Pl5UWdOnVo2bIl48ePJyoqirfeeqvQr2/Tpg179+695HZvb2/H3Vh5i7PJO3Oz9XC4Nd7N8uXWTOEiIiJSZLaHm/OZppnvTMvlbN26lbCwsBKsqORdcw3Uqwe5uQYr3btCQgL89pvdZYmIiJRJtt4t9eSTT9KzZ08iIiI4efIks2bNIjY2liVLlgAwduxYDh48yKeffgrApEmTqFWrFo0aNSIrK4sZM2YQExNDTEyMnV+jWHTtCr/8Asur3sk/E+dYZ2/q1LG7LBERkTLH1nDz559/MmjQIBITEwkODqZp06YsWbKErl2tMV8SExNJSEhw7J+VlcXjjz/OwYMH8fX1pVGjRixcuJBevXrZ9RWKTdeu8O67sCyrg7Vi+XIYPtzeokRERMog2zsUl7aidEgqTSkpUKmSNUjxfmpSM+QkHD4M7u52lyYiImK7MtWhWCzBwXD99VZ7uU9vOH7cmmtKREREikThxon8dTWOZZXvsBoFzLMlIiIiF6dw40Tyws13J1qQiwF/dawWERGRwlO4cSKtW0NAABxJ82U7UbBxIxw9andZIiIiZYrCjRPx9Dw7oN+yaoOsgfyWLrW1JhERkbJG4cbJ5F2aWu5zs9XQpSkREZEiUbhxMnnhZk3i38jAxwo3ubn2FiUiIlKGKNw4mWuvhfBwOJ3lziqfHpCcDFu32l2WiIhImaFw42QMA/IGXF5U/V6roUtTIiIihaZw44Qc4eZke6uh8W5EREQKTeHGCXXubN059dvhYPZSB9avh2PH7C5LRESkTFC4cUIBAXDjjVZ7Uei9Vodinb0REREpFIUbJ+W4NOXXz2osWGBfMSIiImWIwo2T6tnTeow98DfS8bPO3GRl2VuUiIhIGaBw46Tq14fISMg648aKCv3g5ElYtcruskRERJyewo2TyndLeNhQq6FLUyIiIpelcOPEHOHmyPWYAF9/bc03JSIiIpekcOPEOnYEHx9IOOxHnM/1cOAAbN9ud1kiIiJOTeHGifn5nZ1r6utaD1sNXZoSEREpkMKNk+vb13qcn9HNaijciIiIFEjhxsndfLPVufjH3ytzgAjYssW6PCUiIiIXpXDj5KpWhbZtrfbXf3vUasyda19BIiIiTk7hpgzo08d6/Nr9FqsxZ459xYiIiDg5hZsyIC/cxO6L4ATB8P33cOiQvUWJiIg4KYWbMqBuXWjYELKzDRbVedga62bePLvLEhERcUoKN2WE464pvzutRkyMbbWIiIg4M4WbMiLv0tTi3+pyGm9rnqnkZHuLEhERcUIKN2VEy5ZwzTWQlu7GsjoPQW4uzJ9vd1kiIiJOR+GmjHBzg379rPaXgX9NpKm7pkRERC6gcFOG3H679bhgbwMy8YIVK+DIEXuLEhERcTIKN2VIdLR1aSo1zZ2ltR+EnBydvRERETmPwk0Z4uYGt95qtb+sMMxqfPGFfQWJiIg4IYWbMua226zHBXsbcBofWLMGEhLsLUpERMSJKNyUMY5LUyfdWNroX9bKWbPsLUpERMSJKNyUMedemvoq4B6roUtTIiIiDgo3ZVDeXVNf7/wbpz0CYPt22LnT3qJERESchMJNGdSmDYSHw8k0NxY1e9JaOXOmvUWJiIg4CYWbMsjNDe78a4qpGcZdVuOLL6wJNUVERMo5hZsy6q6/Ms3C7eEc97sG4uNh3Tp7ixIREXECCjdlVJMm1pKVZTAn6kVr5Sef2FuUiIiIE1C4KcPyzt7MSP+n1Zg9G06dsq8gERERJ6BwU4YNGACGAat3VCAh/AZITdVM4SIiUu4p3JRhERFw441W+4v6z1uN6dNtq0dERMQZKNyUcY5LUwkdMAGWL4cDB+wsSURExFYKN2Vcv37g7Q0793qx9br7rNvBP/3U7rJERERso3BTxlWoAH37Wu2PQx61GtOna8wbEREptxRuXMDQodbj5z9ey2m/ivDrr9Zs4SIiIuWQreFmypQpNG3alKCgIIKCgoiOjmbx4sUFvmbVqlW0aNECHx8fateuzdSpU0upWufVuTPUqAHHjxvMv/5la+W0afYWJSIiYhNbw014eDivvPIKmzdvZvPmzXTq1Ik+ffqw8xKTQMbHx9OrVy/at2/P1q1befLJJxk9ejQxMTGlXLlzcXeHIUOs9kcZd1iNOXPg6FHbahIREbGLYZrO1TmjYsWKvP7669x7770XbBszZgwLFixg165djnXDhw9n+/btrF+//qLvl5mZSWZmpuN5amoqERERpKSkEBQUVPxfwCbx8VC7NhiGSXyDv1Pz58UwcSL86192lyYiInLVUlNTCQ4OLtTfb6fpc5OTk8OsWbNIT08nOjr6ovusX7+ebt265VvXvXt3Nm/ezJkzZy76mvHjxxMcHOxYIiIiir12ZxAZCZ06gWkafFL7rzFvpk1Tx2IRESl3bA83cXFxBAQE4O3tzfDhw5k3bx4NGza86L5JSUlUq1Yt37pq1aqRnZ3NkSNHLvqasWPHkpKS4lgOuPAYMHkdiz/e0YJcvwDYvRvWrrW3KBERkVJme7ipX78+27ZtY8OGDTz44IMMHjyYn3/++ZL7G4aR73neVbXz1+fx9vZ2dFjOW1zVLbdAcDDsT3BjeYcXrJX//a+9RYmIiJQy28ONl5cXderUoWXLlowfP56oqCjeeuuti+4bGhpKUlJSvnXJycl4eHhQqVKl0ijXqfn6wt13W+0pp4dYDXUsFhGRcsb2cHM+0zTzdQA+V3R0NMuWLcu3bunSpbRs2RJPT8/SKM/pPfCA9bhgdQX+aNwDMjPhww/tLUpERKQU2RpunnzySdasWcP+/fuJi4vjqaeeIjY2loEDBwJWf5m7805FYN0Z9fvvv/Poo4+ya9cuPvroIz788EMef/xxu76C02nUCDp0gNxcgw9q/zXmzXvvQU6OvYWJiIiUElvDzZ9//smgQYOoX78+nTt3ZuPGjSxZsoSuXbsCkJiYSEJCgmP/yMhIFi1aRGxsLM2aNePFF19k8uTJ9OvXz66v4JQefNB6fH9TM85UrAa//w7ffGNvUSIiIqXE6ca5KWlFuU++rMrKgogISE6GmL6fccv8u61hjJcvt7s0ERGRK1Imx7mR4uPlBXljIE45ciu4ucF330EBd6GJiIi4CoUbF3X//WAYsHytL3s6/XWd6p137C1KRESkFCjcuKhateDmm6322/5PWI1PP4UTJ+wqSUREpFQo3Liwhx+2Hqcvv4YT17aB9HR4/317ixIRESlhCjcurFMnaNwY0tMNPor6a2DEt96yehyLiIi4KIUbF2YYZ8/evL2hFTmh18DBgzBrlr2FiYiIlCCFGxc3cCBUqgT7fzdY0GWytfKNNzRbuIiIuCyFGxfn62vdOQUwad8/ICAA4uJg6VJ7CxMRESkhCjflwEMPgYcHrF7nwZbe46yVr79ua00iIiIlReGmHAgPh/79rfZrqcPB3d0a1O/HH+0tTEREpAQo3JQTY8ZYj3MW+7O352jryYQJ9hUkIiJSQhRuyokmTeDvf4fcXHjDa6y1cvZsa1JNERERF6JwU4488ddAxdP/V4XEtrdCTg68+aa9RYmIiBQzhZtypF07uOEGawy/t6551Vo5bZo1fbiIiIiLULgpZ/LO3kxZEklKsxshIwMmTrS3KBERkWKkcFPO/P3v0KgRpKYaTGk6xVr5zjtw9Ki9hYmIiBQThZtyxs3t7J1Tk769ltNNr7cm1Jw0yda6REREiovCTTl0xx1Qowb8+afB9DZTrZWTJ8OJE7bWJSIiUhwUbsohT094/HGr/erSZmQ1bAapqVbAERERKeOKNdz89ttvdOrUqTjfUkrIffdBaCjs32/wSdtp1so337RCjoiISBlWrOEmLS2NVatWFedbSgnx9T1759T/fduSrPpNrMtS77xja10iIiJXS5elyrH774fq1SEhweCj6PetlRMnQlqavYWJiIhcBYWbcszXF8b+NRPDS8uvJ/NvDa1bwtX3RkREyjCFm3Luvvvgmmvgjz8MPmg33Vr52mtw/LitdYmIiFwpj6Ls3Lx5cwzDuOT2U6dOXXVBUrp8fODJJ2HECHh5WUvubdQCn51b4PXX4eWX7S5PRESkyIoUbvr27VtCZYid7r0XXnkFDhwwmNbrY0bvbApvvQWjR1u3VImIiJQhhmmapt1FlKbU1FSCg4NJSUkhKCjI7nKcxn//C8OHQ2ioyb6IjvhuWg0jR8Lbb9tdmoiISJH+fhdrn5vt27fj7u5enG8ppeSee6BmTUhKMni72YfWyv/+F/bvt7UuERGRoir2DsXl7ESQy/DyghdesNrjv6rDsQ594cwZeP55W+sSEREpqmIPNwV1OBbnNnAgNPlrLL/xEe9ZKz/9FHbtsrUuERGRotCt4OLg7g6vvmq1354TRkK3+yA3F55+2t7CREREiqBI4SY1NbXA5eTJkyVVp5SSHj2gY0fIzIRnfF4HNzeYOxe+/97u0kRERAqlSHdLubm5FXjZyTRNDMMgJyenWIorCbpb6vI2bYLrrwfDgG19x9F03vPQujWsX2+tFBERKWVF+ftdpHFuVqxYoT415UCrVnDbbfDVV/BEyhMs8nsdNm60Vtx+u93liYiIFEjj3MhF7d0LDRtCdjasGPwJN30yBCIjrc7F3t52lyciIuVMiY1z4+bmhru7e4GLh0eRTgaJk6pb15o1HODfO+4iN7Q6xMfDu+/aW5iIiMhlFOnMzddff33JbevWrePtt9/GNE0yMjKKpbiSoDM3hZecDHXqwMmT8PE9qxny8Y1QoQL89htUrGh3eSIiUo4U5e/3VV+W2r17N2PHjuWbb75h4MCBvPjii9SoUeNq3rJEKdwUzeuvw3/+Y03L8EvFaAJ/3gj/+hdMnGh3aSIiUo6UyvQLhw4dYtiwYTRt2pTs7Gy2bdvGJ5984tTBRopu9Gj429+saRlebvKFtfKdd+DXX+0tTERE5BKKHG5SUlIYM2YMderUYefOnXz33Xd88803NG7cuCTqE5t5e8OECVZ74rza7Gs/2JqW4bHH7C1MRETkEooUbl577TVq167N//73P2bOnMm6deto3759SdUmTuIf/4DOnSErC/7tMxk8PGDBAvj2W7tLExERuUCRB/Hz9fWlS5cuBc7+PXfu3GIpriSoz82ViYuDZs2s2RhW3vouHeeMhPr1YccOa9ZNERGRElRig/jdfffdGsSvnGrSBB54AKZMgUd2D2dLlf/Dfc8eq//No4/aXZ6IiIiDBvGTQjtyxBr/5sQJeHfgOh76vC0EBcEvv0C1anaXJyIiLqxU7paS8qdyZXjxRav91MJokqO6QmoqPPmkvYWJiIicQ+FGiuTBB6F5czhxwuA/18ywVn78sTXbpoiIiBOwNdyMHz+eVq1aERgYSNWqVenbty979uwp8DWxsbEYhnHBsnv37lKqunxzd4f33rPanyyqytruL4JpwqhRVm9jERERm9kablatWsWIESPYsGEDy5YtIzs7m27dupGenn7Z1+7Zs4fExETHUrdu3VKoWADatIH77rPaDyWMITuggjVr+Acf2FqXiIgIOFmH4sOHD1O1alVWrVpFhw4dLrpPbGwsN910E8ePH6dChQpF/gx1KC4eR45Yd4IfOwYT+67mX/NvhJAQ2L0bqla1uzwREXExZbZDcUpKCgAVCzEpY/PmzQkLC6Nz586sXLnykvtlZmaSmpqab5GrV7kyvPqq1X52eXsONuwKx4/DmDH2FiYiIuWe04Qb0zR59NFHadeuXYFTOYSFhTFt2jRiYmKYO3cu9evXp3Pnzqxevfqi+48fP57g4GDHEhERUVJfodwZOtS6RJWWZvBo2EwwDJg+Hdassbs0EREpx5zmstSIESNYuHAha9euJTw8vEiv7d27N4ZhsGDBggu2ZWZmkpmZ6XiemppKRESELksVk61boWVLqy/xwh5v02vJaGjUyNrg6Wl3eSIi4iLK3GWpUaNGsWDBAlauXFnkYAPQpk0b9u7de9Ft3t7eBAUF5Vuk+DRvDv/6l9V+8KeHSKtUE3buhEmTbK1LRETKL1vDjWmajBw5krlz57JixQoiIyOv6H22bt1KWFhYMVcnhfX881CrFiT84c4zLRZaK8eNg99/t7MsEREpp2wNNyNGjGDGjBl88cUXBAYGkpSURFJSEhkZGY59xo4dy9133+14PmnSJObPn8/evXvZuXMnY8eOJSYmhpEjR9rxFQTw94epU6325OUN2dT8fjh1Ch56yBoDR0REpBTZGm6mTJlCSkoKHTt2JCwszLHMnj3bsU9iYiIJCQmO51lZWTz++OM0bdqU9u3bs3btWhYuXMgtt9xix1eQv3TvDgMHQm6uwbCMtzjj6QeLFsHMmXaXJiIi5YzTdCguLRrnpuQkJ0ODBtbYN690/Y4xy7pY94zv2mU9ioiIXKEy16FYXEPVqjBxotUet6YTv9brZY32l9fjWEREpBQo3Eixuvtu6NwZTp82GOo3k1zDHWbMgCVL7C5NRETKCYUbKVaGAe+/b3UyXrMtiLc7fGVteOABSEuztzgRESkXFG6k2EVGwuuvW+2xP/Rl7zUdISEBnnrK1rpERKR8ULiREvHAA9CpE2RkGAytMJdcDHj7bVi/3u7SRETExSncSIlwc4MPP4SAAFi7M4TJrWZYY97cdx9kZdldnoiIuDCFGykxtWqdvTz15E8D2BtyPfz8M7zwgq11iYiIa1O4kRL1wAPQpYt1eeqeqgvJwQ1eeQV++MHu0kRExEUp3EiJMgz44AMIDITv91TmreafQE4ODB4M50yzISIiUlwUbqTE1awJb7xhtZ/aNZA9ldvC7t3w9NP2FiYiIi5J4UZKxbBh0LWrNbjfXSH/4wwe8OabsHq13aWJiIiLUbiRUmEY8NFHEBICm/dWYFzUfOvuqSFDNLifiIgUK4UbKTXh4TBtmtUev6MXa6r2g/h4+Pe/7S1MRERcisKNlKpbb7X6EpumwSDjM1IIgqlT4dtv7S5NRERchMKNlLrJk60pGn7/05eR9ZdbK++9F44ft7cwERFxCQo3UuqCgqyJwt3cYMaeVswKfQQOHoQHH7T64YiIiFwFhRuxxQ03nL0TfHja6yS41YLZs+HTT22tS0REyj6FG7HN009D69aQkubB3TVjrdGLR4yAX3+1uzQRESnDFG7ENp6e1uUpf39YFV+TV2tNhfR0uPNOOHPG7vJERKSMUrgRW9WpA2+/bbWfSbiPNQE9YdMmeO45ewsTEZEyS+FGbDdkCNx1F+TmGgzwmsMRKlmTa65caXdpIiJSBinciO0MA6ZMgfr14eAxPwZHrCDXBAYNgmPH7C5PRETKGIUbcQoBAfDll+DjA4sONGVC5Ves28OHDdPt4SIiUiQKN+I0mjaFt96y2k8e/zfr3dvB3Lnw3//aW5iIiJQpCjfiVIYNgzvugOwcgzsCF3KMEHj4YfjxR7tLExGRMkLhRpyKYVgnaurUgYQTQdxTbTFmVhbcdhucOGF3eSIiUgYo3IjTCQqy+t94ecGCP1szMeT/YN8+uOce9b8REZHLUrgRp9S8OUyaZLXHpD7JSo+uMH/+2ZUiIiKXoHAjTmv4cLj7bsjJMejvM58DhMN//gPr1tldmoiIODGFG3FahgFTp0KzZnA4zY9bK64gM9sN+veHI0fsLk9ERJyUwo04NV9f627wkBD44VhdRgd/An/8kTeksd3liYiIE1K4EacXGQkzZ1pncqal3MEHHsPh22/hhRfsLk1ERJyQwo2UCd27w4svWu0RvMMPtILnn4evv7a3MBERcToKN1JmjB0LffpAVrY7/QK+JZkq1uWpn3+2uzQREXEiCjdSZri5wSefQL168EdaCP2ClpOZlmUlHg3wJyIif1G4kTIlONga7iYoCNamNuVB/88wf/0V7rwTcnLsLk9ERJyAwo2UOQ0awOzZ1pmcj9NvZ4LHE7B4MTz9tN2liYiIE1C4kTKpRw94802r/Z+cl/mGm+GVV6x5G0REpFxTuJEya9QoeOABME2DOz2/Io7G1vxT27bZXZqIiNhI4UbKLMOAt9+GTp0g7YwPvX2WkXzKH26+GQ4etLs8ERGxicKNlGmenvDVV1CnDvx+OpR/+i4h8+Bh6N0b0tLsLk9ERGygcCNlXsWK8L//WXdSrcu4jnu8vyB36zYYMEB3UImIlEMKN+IS6teHOXPAwwNmZvbjCfc3rMTz6KN2lyYiIqVM4UZcRpcu8NFHVvv1nEeZzCiYPNnqmCMiIuWGwo24lEGD4OWXrfYjxlvEcAs88oh1FkdERMoFhRtxOU88AQ8+aN0iPtB9Fmtyb4A77oAff7S7NBERKQW2hpvx48fTqlUrAgMDqVq1Kn379mXPnj2Xfd2qVato0aIFPj4+1K5dm6lTp5ZCtVJW5N0i3rcvZOZ48g+PRfycXgN69oTffrO7PBERKWG2hptVq1YxYsQINmzYwLJly8jOzqZbt26kp6df8jXx8fH06tWL9u3bs3XrVp588klGjx5NTExMKVYuzs7dHb74AqKj4UR2ID08v+Ngsgd06wZJSXaXJyIiJcgwTdO0u4g8hw8fpmrVqqxatYoOHTpcdJ8xY8awYMECdu3a5Vg3fPhwtm/fzvr16y/7GampqQQHB5OSkkJQUFCx1S7O6ehRuOEG+OUXaOj5C6vO3EDlZhEQG2vdOy4iImVCUf5+O1Wfm5SUFAAqVqx4yX3Wr19Pt27d8q3r3r07mzdv5syZMxfsn5mZSWpqar5Fyo9KleDbb+Gaa+DnM/Xo7vEdKdv2WdesTp+2uzwRESkBThNuTNPk0UcfpV27djRu3PiS+yUlJVGtWrV866pVq0Z2djZHjhy5YP/x48cTHBzsWCIiIoq9dnFutWrB8uVQuTL8mB3F392XkB77AwwcCNnZdpcnIiLFzGnCzciRI9mxYwczZ8687L6GYeR7nndl7fz1AGPHjiUlJcWxHDhwoHgKljLl2mth6VLrStT3OdHcYswnc+7/4N57ITfX7vJERKQYOUW4GTVqFAsWLGDlypWEh4cXuG9oaChJ53UITU5OxsPDg0qVKl2wv7e3N0FBQfkWKZ+aN4fFi8HfH5aaXRnALLI//RxGjADn6XomIiJXydZwY5omI0eOZO7cuaxYsYLIyMjLviY6Opply5blW7d06VJatmyJp6dnSZUqLiI6Gr7+Gry9YR7/5B4+Jnfqf+HxxxVwRERchK3hZsSIEcyYMYMvvviCwMBAkpKSSEpKIiMjw7HP2LFjufvuux3Phw8fzu+//86jjz7Krl27+Oijj/jwww95/PHH7fgKUgZ17mzNJO7uDjMYxAP8l9yJb8Jzz9ldmoiIFANbw82UKVNISUmhY8eOhIWFOZbZs2c79klMTCQhIcHxPDIykkWLFhEbG0uzZs148cUXmTx5Mv369bPjK0gZ1bs3zJgBbm7wAcMYxvvkvvh/8NJLdpcmIiJXyanGuSkNGudGzjVzJtx1l9WneDDT+ZB7cX/+OXj2WbtLExGRcxTl77dHKdUk4pQGDLAuT915J3ySM4Qc3Jn+3BDcc3Oty1QXuQNPREScm8KNlHu3325dnhowAGZkDyIHdz59/m48cnPh+ecVcEREyhiFGxHg1lutMzi33w4zs+8kFzdmvHgXHtnZVj8cBRwRkTLDKca5EXEG//wnzJkDnp4wmzu4lTmcHj8RHn5YA/2JiJQhCjci5+jTB2JirHFwvqYvPVlM6tvT4Z57NFWDiEgZoXAjcp7eva2RjAMDIZab6MQKDn+6CG67DTIz7S5PREQuQ+FG5CJuuglWrrQm29xCS9rxPQnzt8DNN0Namt3liYhIARRuRC6hRQtYuxZq1IBfqEdbYx27lv8BnTrBn3/aXZ6IiFyCwo1IAerXtwJOgwbwhxlOe2MtGzcZ1iRVv/xid3kiInIRCjcilxERAatXQ6tWcNSsxE1GLHPjm8ENN8C6dXaXJyIi51G4ESmEypVhxQro1QsyTF9uZQ5vHB2C2akzzJ1rd3kiInIOhRuRQgoIgK+/hhEjwMSNf/MGD2a+SXa//vDWW3aXJyIif1G4ESkCDw94+22YNAkMw+S/DKc3C0h95BkYPVpj4YiIOAGFG5EiMgxr0OJ58wz8/EyW0JN2rCXh7fnQowccPWp3iSIi5ZrCjcgV6tMHVq82CA2FOJrSki2s+u4MXH897Nxpd3kiIuWWwo3IVWjRAjZuhGbN4DBV6MJy3t7XC7N1G6uDjoiIlDqFG5GrVKMGfP893HknZOPJaN7mnvS3Od23P7z4Ipim3SWKiJQrCjcixcDPD2bMgAkTwM3N5BOG0J41HHh2GtxyC5w4YXeJIiLlhsKNSDExDHj0UVi61KBSJdhMK1qwhZXzT8B118GWLXaXKCJSLijciBSzzp1h8+a8fjhV6cJyno8fRE50O3j3XV2mEhEpYQo3IiWgVi2rH86QIZCLO+N4nm5n/kfiyP+DO+6A1FS7SxQRcVkKNyIlxM8PPv4YPv0U/P1NVtCZZmxj2ZfHoGVL2LbN7hJFRFySwo1ICRs0CDZvNmjSBJKpRne+5am9g8luFQ2vvQY5OXaXKCLiUhRuRErBtdda4+E88IA1L9XLPEX77BX8OmYadOoEv/9ud4kiIi5D4UaklPj6wtSpMGsWBAWZbCCaKLYzdXUDzCZN4bPP1NlYRKQYKNyIlLL+/WHHDoObboJT+PMgU/n7yZkk3v0fa6PmphIRuSoKNyI2qFkTli+HN98Eb2+TxfSiMT/x1VcmNGwIX32lszgiIldI4UbEJm5u8Mgj8OOPBtddB8eoxO18xR3Jb/Hn7SPhn/+EQ4fsLlNEpMxRuBGxWcOGsH49PPMMuLubzOYOGrCL6V9XwGzQEN5/X2dxRESKQOFGxAl4ecELL8DGjQbNm8NxKnIP0+maOoff7n/FuqNq7167yxQRKRMUbkScSIsW8MMP1vA3Pj4m39GFJsTxWmwrshtFwdNPw6lTdpcpIuLUFG5EnIyHB/z73xAXZ9CpE2Tgxxheo8WZ9ax5aRU0aABz5+pSlYjIJSjciDipOnWsO6o+/hhCQkx2EEUH1jAw4WUO9RsJPXrAnj12lyki4nQUbkScmGFYk2/+8ovBAw+AYZh8wUDqs4fXlzYlq/F11mmeEyfsLlVExGko3IiUAZUrW6Mbb9pk0KYNpBHIf3idptlbWPzGT5i1/waTJkFmpt2liojYTuFGpAxp0QK+/x6mT4dq1WAP19KLxXQ9Ppsf//Wp1R9n9mz1xxGRck3hRqSMcXODwYOt7jaPPw5eXtZdVS34kbviX2D/HWOgTRuIjbW7VBERWyjciJRRwcHw+uuwZ4/BwIHWus+5i/rs4fEfbuPYTbdA586wdq29hYqIlDKFG5EyrlYtmDEDtmyxxvrLwpsJPE4k8Ty3ogMn2t8M3bpZwyCLiJQDCjciLuK666xbxxcvhqZNIZVgXuA5arGfF5a1IeWGHtCrF2zcaHepIiIlSuFGxIUYhjX8zdat1sTijRpBChV4jheIJJ6XFjfnZJsucNNN8O236ngsIi5J4UbEBbm5wa23wo4dMGsWXHutNV/V07xEDRJ4OrYzyT0GWad7Zs+G7Gy7SxYRKTYKNyIuzM0N+veHn36Czz+H+vXhBCG8xNPU5HdGbruX+DuesNLPe+9BWprdJYuIXDWFG5FywN0d7rwTdu6EmBho1QpO48u7jKQuexn42/NsGzENwsPhsccgPt7ukkVErpjCjUg54u4Ot9xi9SlesQK6d4ccPPiCgTRnGzemfE3MxP1k164HffvCypXqlyMiZY7CjUg5ZBhWn+IlS+DHH2HAAPDwMFnNjdxKDH/jV177uh7HOvWDxo3hrbfg2DG7yxYRKRRbw83q1avp3bs31atXxzAM5s+fX+D+sbGxGIZxwbJ79+7SKVjEBTVvDl98Afv3Gzz1lDWPVQI1GcNrhPMHw35+hB8e+RwzrDoMGgRr1uhsjog4NVvDTXp6OlFRUbzzzjtFet2ePXtITEx0LHXr1i2hCkXKj2uugf/7PzhwAD76CKKiIAM/PmAYrfmBZlkbeWdGMCc69IaGDeGNN+DQIbvLFhG5gGGazvFPMMMwmDdvHn379r3kPrGxsdx0000cP36cChUqXNHnpKamEhwcTEpKCkFBQVdWrEg5YJrWzA3TpsGcOSanTxsA+JDBbXzFMN6nnbEOo0tn64zOP/8JAQE2Vy0irqoof7/LZJ+b5s2bExYWRufOnVm5cmWB+2ZmZpKamppvEZHLMwxo3x4++wwOHTKYPBmaNLHusvqMu+nAGhqaP/Hyspbsv/sZCA2Fu++GZcsgJ8fu8kWkHCtT4SYsLIxp06YRExPD3LlzqV+/Pp07d2b16tWXfM348eMJDg52LBEREaVYsYhrCAmBUaNg+3bYsAHuvRf8/WE3DXiKl4lkP+3TFzP1Mz+OdrsDIiLg4Yet/jm5uXaXLyLlTJm6LHUxvXv3xjAMFixYcNHtmZmZZGZmOp6npqYSERGhy1IiV+nkSWuKh88/h5UrTUzTumzlSRY9WcxAPqc33+AbFmLdf37rrdapIHd3mysXkbLI5S9LnatNmzbs3bv3ktu9vb0JCgrKt4jI1QsMhKFD4bvv4MABg9dfh2bN4AxeLKAP/fmSKhymf+KbfPluMmk33QzVq8ODD1qXrrKy7P4KIuKiyny42bp1K2FhYXaXIVKuXXMNPP64NWHnTz/B2LFQsyakE8CX9HcEnb7J/+WzqWmc6Habdc/5rbfCJ59AcrLdX0FEXIitl6XS0tL49ddfAauT8MSJE7npppuoWLEiNWrUYOzYsRw8eJBPP/0UgEmTJlGrVi0aNWpEVlYWM2bM4JVXXiEmJoZbbrmlUJ+pu6VESodpwqZN1nQPMTHw229nt3mSRWe+ozff8HcWUtM4AK1bw803Q+/eVs9lw7CveBFxOkX5+21ruMm7tft8gwcPZvr06QwZMoT9+/cTGxsLwGuvvca0adM4ePAgvr6+NGrUiLFjx9KrV69Cf6bCjUjpM01rhvK8oPPzz/m3NyaOv7OQv7OQaNbjEVYVunSBrl2tJTTUnsJFxGmUmXBjB4UbEfvt3g3z58PChbBuXf4bqkI4Rg+W0ItFdGE5ofxpncnJCzodOoCfn221i4g9FG4KoHAj4lyOHoVvv7WCzpIlF05h1Zg4urCcriyjA6sJ8MyypjXv0MFa2rYF/X9ZxOUp3BRA4UbEeeXkWOPoLFwIS5dak3qe+18oD84QzXq6sJyOxHI9P+DjdsaaICsv7LRrZ3VWFhGXonBTAIUbkbLj6FFYsQKWL7eWffvyb/cik+v5gQ6spj1ruIF1BHHSmvuqTRurk3KbNtCokcbXESnjFG4KoHAjUnbt22eFnO++g9WrISkp/3Y3cmjGNkfYiWY9YSRZwym3anU28LRuDRpCQqRMUbgpgMKNiGswTfj1V2uGhzVrrLBz/pkdgAjjAK3NDbRmI63ZSAu24EeGNUXEdddZS/Pm1mP16roFXcRJKdwUQOFGxHUdPHg27KxZYw0oeP5/4dzJpglx+cJOA3bhSTZUqZI/7DRrBrVr65KWiBNQuCmAwo1I+XHyJGzeDBs3nl0SEy/cz9vIpLEZR3O20pytNGMbUWzHn1Pg62v14WnUCBo3PruEh+ssj0gpUrgpgMKNSPllmvDHH2eDzg8/WFNGnDx54b4GudQz9tLc/NEReBqxk+ocwgDr9vNzA8+110L9+tblLrcyP7ONiNNRuCmAwo2InCs3F+LjrZCTt2zbdvEzPAAV3FJomLuTRvxEI3Y6llCSrNDj4wN160K9elbYOfexYsVS/GYirkXhpgAKNyJSGH/+mT/sbN9udWDOybn4/iFuJ2hk/kQj8ycasIu67KUev1CL/Xjw14sqVbJCTu3a1hIZebZdvbr69ogUQOGmAAo3InKlMjNhzx7YuTP/8ttv+aeQOJeHkU1t99+pl/2zI/DU4xfqspdrOIgbf/0n2NMTatU6G3jOfYyIsDo7q4+PlGMKNwVQuBGR4paRkT/0/PKLtfz6q7XtUnzdTlPHYz9/O7ObSHMfkcQ7llrstzo05/H2tjoxR0TkX85dFxKiACQuS+GmAAo3IlJacnOt29N/+QX27j0bevbutcbkyc4u+PVVPY9ZYeeMdXkrL/jU5HfC+cMar+dcfn5WyAkLy7+EhuZ/XqGCQpCUOQo3BVC4ERFncOYM7N9vhZ19+6x2fPzZJSXl8u9R0TOVCI9EwrN/J+LMb0RwgHD+cDyG8we+nL7whd7eFwaeatWsS1/nLxUrqi+QOAWFmwIo3IhIWXD8+Nmgc37wSUiA9PTCvU8l75NEeCYRzh+EZf1uLSQSSpLjMZQkvMm6+Bu4uVkBp0oVqFr14gGocmVrn5AQ6zEwUGeGpNgp3BRA4UZEyjrTtM7sHDhgjdtzsccDB+DUqcu/V54Q73TCvI4S5vYnobmHCMtKIDRzvyMAVSWZKhymEkfP3v11Ke7uVtDJCzvnBp9LPQYHW4u/v4KRXJTCTQEUbkSkPDBNOHHCCjt5gScpyRq/J+8xr511iZM2F2MYJiE+GVTxTqWK+3GqcJgqOUlUyT5ElYwDVMlJdAShKhymMkfw4kzhP8DNzRogMTj48o8FbfPyKvIxE+emcFMAhRsRkbNM07oEdn7gOT8EHT4Mx45dOFdXYQT7ZlLZL4OKXmlU9EilonGciuZRKuYkUzHrTypmHLSW3MNU5BgVOUYIx635vq6UpycEBJxd/P3zP7+SxddXo0/bqCh/vz1KqSYREXFChnH2ylHDhgXvm5MDR49aQacwy5Ej1mtSMrxJyfDmNyoUqbYA32wq+mdS0fc0FX3Sqeh5koruVjiqYB4nOPsowWeOUCHzT4IzkghOP0SFjEMEk4L/mXSM48et5Fac/PxKfvH0LN6ayyGFGxERKRR3d6tPcdWqhds/N9e6NJYXdI4ft87+XG45ccI6Q5SW4UFahgcJ+AOVilirSXBALsEB2QT7ZhHsk0UFnwyCPTMI9kingvtJgo1UgkmxglLOMYLPHCE46zBBmYcJzEjGPz0ZIz3N6r2dd8rq1KmidWa6Eh4elw4+vr7Wcm77/OdFabtokFK4ERGREpF3o1XFitb0WoWVk2N1mD4/9OSFo6NHrQCUknJ2Ofd5djbk5BgcS3HnWIo74H1F9RuGdTUqqLpJoL9JoF8Ogb7ZBPqeIcg7i0DvTAI9TxPokUGQ+ykC3dIJNNII5CRBpBKYm0JgznECzxwn8MwxPE6nnQ1HF1vyhrnOzobUVGspae7uVxeOLhW0AgKsCWVtonAjIiJOxd39bCgqKtO0csKlgk9BoSivffKklTNM02qfPGkABuAGeAK+V/S9fH2tu+TzlqDK5zwPMAn0zyHA6wyB3lkEeGYS4JFBoHsGAW6nCDCs4BRAGgG5qQSaqXhmpVtfNiPj7HLu84LaeXJyIC3NWopT1arWBG02UbgRERGXYRhW32F/f2su0ithmlYGOHnSOnliBZz87aI8z7sbLS9bJCdftHKsP8keFDY8eXlZwSivv3O+dsVLrA+EAH+TAO8zBHqeJuCv8BTofgp/Mw33rCsIShdrV658ZQe/mCjciIiInMMwznZxqVbt6t8vK+vyYSg19ewJlLQ0a92l2nlhKSvLukR39GiRvyHg9deS/66jvCtKFw1Fee0gCKhe8D5BQVDIrlklQuFGRESkBHl5QaVK1lIcsrKsPs4FBaDLtc99fvKkdXUKznb/ufjZpcKrVMnqRG4XhRsREZEyxMvLWkJCiuf9TBMyM688KF2sHRxcPLVdKYUbERGRcswwwMfHWoqrq4zdwwNrqEUREREpVnZPD6ZwIyIiIi5F4UZERERcisKNiIiIuBSFGxEREXEpCjciIiLiUhRuRERExKUo3IiIiIhLUbgRERERl6JwIyIiIi5F4UZERERcisKNiIiIuBSFGxEREXEpCjciIiLiUjzsLqC0mX/Nw56ammpzJSIiIlJYeX+38/6OF6TchZuTJ08CEBERYXMlIiIiUlQnT54kODi4wH0MszARyIXk5uZy6NAhAgMDMQyjWN87NTWViIgIDhw4QFBQULG+t6vRsSoaHa/C07EqGh2vwtOxKrySOFamaXLy5EmqV6+Om1vBvWrK3ZkbNzc3wsPDS/QzgoKC9MMvJB2rotHxKjwdq6LR8So8HavCK+5jdbkzNnnUoVhERERcisKNiIiIuBSFm2Lk7e3Nc889h7e3t92lOD0dq6LR8So8Haui0fEqPB2rwrP7WJW7DsUiIiLi2nTmRkRERFyKwo2IiIi4FIUbERERcSkKNyIiIuJSFG6KyXvvvUdkZCQ+Pj60aNGCNWvW2F2SUxg3bhyGYeRbQkNDHdtN02TcuHFUr14dX19fOnbsyM6dO22suPSsXr2a3r17U716dQzDYP78+fm2F+bYZGZmMmrUKCpXroy/vz//+Mc/+OOPP0rxW5SOyx2rIUOGXPA7a9OmTb59ysuxGj9+PK1atSIwMJCqVavSt29f9uzZk28f/bbOKszx0u/LMmXKFJo2beoYmC86OprFixc7tjvT70rhphjMnj2bRx55hKeeeoqtW7fSvn17evbsSUJCgt2lOYVGjRqRmJjoWOLi4hzbXnvtNSZOnMg777zDpk2bCA0NpWvXro45wFxZeno6UVFRvPPOOxfdXphj88gjjzBv3jxmzZrF2rVrSUtL4+abbyYnJ6e0vkapuNyxAujRo0e+39miRYvybS8vx2rVqlWMGDGCDRs2sGzZMrKzs+nWrRvp6emOffTbOqswxwv0+wIIDw/nlVdeYfPmzWzevJlOnTrRp08fR4Bxqt+VKVft+uuvN4cPH55v3bXXXms+8cQTNlXkPJ577jkzKirqottyc3PN0NBQ85VXXnGsO336tBkcHGxOnTq1lCp0DoA5b948x/PCHJsTJ06Ynp6e5qxZsxz7HDx40HRzczOXLFlSarWXtvOPlWma5uDBg80+ffpc8jXl9ViZpmkmJyebgLlq1SrTNPXbupzzj5dp6vdVkJCQEPODDz5wut+VztxcpaysLLZs2UK3bt3yre/WrRvr1q2zqSrnsnfvXqpXr05kZCR33HEH+/btAyA+Pp6kpKR8x87b25sbb7yx3B+7whybLVu2cObMmXz7VK9encaNG5fL4xcbG0vVqlWpV68ew4YNIzk52bGtPB+rlJQUACpWrAjot3U55x+vPPp95ZeTk8OsWbNIT08nOjra6X5XCjdX6ciRI+Tk5FCtWrV866tVq0ZSUpJNVTmP1q1b8+mnn/Ltt9/y/vvvk5SUxA033MDRo0cdx0fH7kKFOTZJSUl4eXkREhJyyX3Ki549e/L555+zYsUKJkyYwKZNm+jUqROZmZlA+T1Wpmny6KOP0q5dOxo3bgzot1WQix0v0O/rXHFxcQQEBODt7c3w4cOZN28eDRs2dLrfVbmbFbykGIaR77lpmhesK4969uzpaDdp0oTo6Gj+9re/8cknnzg65OnYXdqVHJvyePz69+/vaDdu3JiWLVtSs2ZNFi5cyC233HLJ17n6sRo5ciQ7duxg7dq1F2zTb+tClzpe+n2dVb9+fbZt28aJEyeIiYlh8ODBrFq1yrHdWX5XOnNzlSpXroy7u/sFqTM5OfmCBCvg7+9PkyZN2Lt3r+OuKR27CxXm2ISGhpKVlcXx48cvuU95FRYWRs2aNdm7dy9QPo/VqFGjWLBgAStXriQ8PNyxXr+ti7vU8bqY8vz78vLyok6dOrRs2ZLx48cTFRXFW2+95XS/K4Wbq+Tl5UWLFi1YtmxZvvXLli3jhhtusKkq55WZmcmuXbsICwsjMjKS0NDQfMcuKyuLVatWlftjV5hj06JFCzw9PfPtk5iYyE8//VTuj9/Ro0c5cOAAYWFhQPk6VqZpMnLkSObOncuKFSuIjIzMt12/rfwud7wupjz/vs5nmiaZmZnO97sq1u7J5dSsWbNMT09P88MPPzR//vln85FHHjH9/f3N/fv3212a7R577DEzNjbW3Ldvn7lhwwbz5ptvNgMDAx3H5pVXXjGDg4PNuXPnmnFxceaAAQPMsLAwMzU11ebKS97JkyfNrVu3mlu3bjUBc+LEiebWrVvN33//3TTNwh2b4cOHm+Hh4eby5cvNH3/80ezUqZMZFRVlZmdn2/W1SkRBx+rkyZPmY489Zq5bt86Mj483V65caUZHR5vXXHNNuTxWDz74oBkcHGzGxsaaiYmJjuXUqVOOffTbOutyx0u/r7PGjh1rrl692oyPjzd37NhhPvnkk6abm5u5dOlS0zSd63elcFNM3n33XbNmzZqml5eXed111+W7jbA869+/vxkWFmZ6enqa1atXN2+55RZz586dju25ubnmc889Z4aGhpre3t5mhw4dzLi4OBsrLj0rV640gQuWwYMHm6ZZuGOTkZFhjhw50qxYsaLp6+tr3nzzzWZCQoIN36ZkFXSsTp06ZXbr1s2sUqWK6enpadaoUcMcPHjwBcehvByrix0nwPz4448d++i3ddbljpd+X2cNHTrU8XeuSpUqZufOnR3BxjSd63dlmKZpFu+5IBERERH7qM+NiIiIuBSFGxEREXEpCjciIiLiUhRuRERExKUo3IiIiIhLUbgRERERl6JwIyIiIi5F4UZERERcisKNiAjWbMbz58+3uwwRKQYKNyJiuyFDhmAYxgVLjx497C5NRMogD7sLEBEB6NGjBx9//HG+dd7e3jZVIyJlmc7ciIhT8Pb2JjQ0NN8SEhICWJeMpkyZQs+ePfH19SUyMpKvvvoq3+vj4uLo1KkTvr6+VKpUifvvv5+0tLR8+3z00Uc0atQIb29vwsLCGDlyZL7tR44c4Z///Cd+fn7UrVuXBQsWlOyXFpESoXAjImXCM888Q79+/di+fTt33XUXAwYMYNeuXQCcOnWKHj16EBISwqZNm/jqq69Yvnx5vvAyZcoURowYwf33309cXBwLFiygTp06+T7j+eef5/bbb2fHjh306tWLgQMHcuzYsVL9niJSDIp9nnERkSIaPHiw6e7ubvr7++dbXnjhBdM0TRMwhw8fnu81rVu3Nh988EHTNE1z2rRpZkhIiJmWlubYvnDhQtPNzc1MSkoyTdM0q1evbj711FOXrAEwn376acfztLQ00zAMc/HixcX2PUWkdKjPjYg4hZtuuokpU6bkW1exYkVHOzo6Ot+26Ohotm3bBsCuXbuIiorC39/fsb1t27bk5uayZ88eDMPg0KFDdO7cucAamjZt6mj7+/sTGBhIcnLylX4lEbGJwo2IOAV/f/8LLhNdjmEYAJim6WhfbB9fX99CvZ+np+cFr83NzS1STSJiP/W5EZEyYcOGDRc8v/baawFo2LAh27ZtIz093bH9+++/x83NjXr16hEYGEitWrX47rvvSrVmEbGHztyIiFPIzMwkKSkp3zoPDw8qV64MwFdffUXLli1p164dn3/+OT/88AMffvghAAMHDuS5555j8ODBjBs3jsOHDzNq1CgGDRpEtWrVABg3bhzDhw+natWq9OzZk5MnT/L9998zatSo0v2iIlLiFG5ExCksWbKEsLCwfOvq16/P7t27AetOplmzZvHQQw8RGhrK559/TsOGDQHw8/Pj22+/5eGHH6ZVq1b4+fnRr18/Jk6c6HivwYMHc/r0ad58800ef/xxKleuzK233lp6X1BESo1hmqZpdxEiIgUxDIN58+bRt29fu0sRkTJAfW5ERETEpSjciIiIiEtRnxsRcXq6ei4iRaEzNyIiIuJSFG5ERETEpSjciIiIiEtRuBERERGXonAjIiIiLkXhRkRERFyKwo2IiIi4FIUbERERcSn/Dy1HCHjgqgQgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 300\n",
    "\n",
    "# Initialize a new network\n",
    "z_size = hidden_size + vocab_size # Size of concatenated hidden + input vector\n",
    "params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "        # Initialize hidden state and cell state as zeros\n",
    "        h = np.zeros((hidden_size, 1))\n",
    "        c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss, _ = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "        # Initialize hidden state and cell state as zeros\n",
    "        h = np.zeros((hidden_size, 1))\n",
    "        c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "        # Update parameters\n",
    "        params = update_parameters(params, grads, lr=1e-3)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss\n",
    "                \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if i % 50 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "    \n",
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "h = np.zeros((hidden_size, 1))\n",
    "c = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "\n",
    "# Print example\n",
    "print('Input sentence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18958651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15731e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "Input sentence:\n",
      "['a', 'a', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS', 'EOS', 'EOS']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'b']\n",
      "Target sequence:\n",
      "['b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS', 'EOS', 'EOS']\n",
      "Input sentence:\n",
      "['a', 'a', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS', 'EOS', 'EOS']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS', 'EOS']\n",
      "Input sentence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "        # Initialize hidden state and cell state as zeros\n",
    "        h = np.zeros((hidden_size, 1))\n",
    "        c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "        output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]\n",
    "        print('Input sentence:')\n",
    "        print(inputs)\n",
    "        print('Target sequence:')\n",
    "        print(targets)\n",
    "        print('Predicted sequence:')\n",
    "        print([idx_to_word[np.argmax(output)] for output in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533b671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9503f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35916792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
